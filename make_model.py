# -*- coding: utf-8 -*-
"""coin.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A6kvp2rKgUVc33dhS1ohwMmePMizyfcp

# This file is not used in time_series_test. Please note
"""


import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
import joblib
from sklearn.preprocessing import StandardScaler

df = pd.read_csv(r"/content/drive/MyDrive/new/BTC_USDT_1h_2023_3_17_to_current.csv")

df['returns'] = df['close'].pct_change()
df['target'] = np.where(df['returns'] > 0, 1, 0)
df.fillna(0, inplace=True)

features = df[['close', 'returns', 'high', 'low']].values
targets = df['target'].values

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)
scaler_filename = "/content/drive/MyDrive/new/4k_scaler.save"
joblib.dump(scaler, scaler_filename)
print("Scaler saved at", scaler_filename)

n_splits = 5
look_back = 24
tscv = TimeSeriesSplit(n_splits=n_splits)


fold = 1
for train_index, val_index in tscv.split(features_scaled):
    print(f"\nFold {fold} 시작:")


    train_features, val_features = features_scaled[train_index], features_scaled[val_index]
    train_targets, val_targets = targets[train_index], targets[val_index]


    train_generator = TimeseriesGenerator(train_features, train_targets, length=look_back, batch_size=20)
    val_generator = TimeseriesGenerator(val_features, val_targets, length=look_back, batch_size=1)


    model = Sequential()
    model.add(LSTM(100, activation='tanh', input_shape=(look_back, 4), return_sequences=True))
    model.add(LSTM(100, activation='tanh'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


    early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='min', restore_best_weights=True)
    model_checkpoint = ModelCheckpoint(
        filepath=f"/content/drive/MyDrive/model_ag_no_test/1213segment_4k/fold{fold}_model_epoch_{{epoch:02d}}.keras",
        monitor='val_loss',
        mode='min',
        save_best_only=True,
        verbose=1)


    history = model.fit(train_generator, epochs=100, validation_data=val_generator, callbacks=[early_stopping, model_checkpoint])

    print(f"Fold {fold} 학습 완료.")
    fold += 1

import pandas as pd
from keras.models import Sequential
from keras.layers import LSTM, Dense
import numpy as np
from keras.preprocessing.sequence import TimeseriesGenerator
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from keras.callbacks import ModelCheckpoint
from keras.layers import Dropout
from sklearn.preprocessing import StandardScaler

df = pd.read_csv(r"/content/drive/MyDrive/btc_usdt_future_1h_2021.csv")

df_close = df[['close']].values

scaler = StandardScaler()
data_scaled = scaler.fit_transform(df_close)

train_data, test_data = train_test_split(data_scaled, test_size=0.2, shuffle=False)
train_data, val_data = train_test_split(train_data, test_size=0.25, shuffle=False)

look_back = 8
train_generator = TimeseriesGenerator(train_data, train_data[:, 0], length=look_back, batch_size=20)
val_generator = TimeseriesGenerator(val_data, val_data[:, 0], length=look_back, batch_size=1)
test_generator = TimeseriesGenerator(test_data, test_data[:, 0], length=look_back, batch_size=1)

model = Sequential()
model.add(LSTM(200, activation='tanh', input_shape=(look_back, 1), return_sequences=True))
#model.add(Dropout(0.2))
model.add(LSTM(200, activation='tanh'))
#model.add(Dropout(0.2))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')

from keras.callbacks import EarlyStopping, ModelCheckpoint
early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)

model_checkpoint = ModelCheckpoint(
    'best_model.tf',
    monitor='val_loss',
    mode='min',
    save_best_only=True,
    verbose=1,
)

history = model.fit(
    train_generator,
    epochs=100,
    verbose=1,
    validation_data=val_generator,
    callbacks=[early_stopping,model_checkpoint]
)

actual = df_close[len(train_data) + len(val_data) + look_back:][:len(test_generator)]
prediction = model.predict(test_generator)
prediction = scaler.inverse_transform(prediction)

mae = mean_absolute_error(actual, prediction)
mse = mean_squared_error(actual, prediction)
rmse = np.sqrt(mse)
print(f'Mean Absolute Error (MAE): {mae}')
print(f'Mean Squared Error (MSE): {mse}')
print(f'Root Mean Squared Error (RMSE): {rmse}')

#여러 평가지표 활용학습

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing.sequence import TimeseriesGenerator

def calculate_rsi(data, window=14):
    delta = data.diff()
    gain = delta.where(delta > 0, 0)
    loss = (-delta).where(delta < 0, 0)

    avg_gain = gain.rolling(window=window, min_periods=1).mean()
    avg_loss = loss.rolling(window=window, min_periods=1).mean()

    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

def calculate_macd(data, short_window=12, long_window=26, signal_window=9):
    short_ema = data.ewm(span=short_window, adjust=False).mean()
    long_ema = data.ewm(span=long_window, adjust=False).mean()
    macd = short_ema - long_ema
    signal_line = macd.ewm(span=signal_window, adjust=False).mean()
    return macd, signal_line

def calculate_obv(data, volume):
    obv = np.where(data > data.shift(1), volume, -volume).cumsum()
    return obv

def calculate_atr(high, low, close, window=14):
    high_low = high - low
    high_close = np.abs(high - close.shift())
    low_close = np.abs(low - close.shift())
    ranges = pd.concat([high_low, high_close, low_close], axis=1)
    true_range = np.max(ranges, axis=1)
    atr = true_range.rolling(window=window).mean()
    return atr

df = pd.read_csv(r"/content/drive/MyDrive/btc_usdt_future_1h_2021.csv")
df['returns'] = df['close'].pct_change()
df['RSI'] = calculate_rsi(df['close'])
macd, signal_line = calculate_macd(df['close'])
df['MACD'] = macd
df['MACD_Signal'] = signal_line
df['OBV'] = calculate_obv(df['close'], df['volume'])
df['ATR'] = calculate_atr(df['high'], df['low'], df['close'])
df['SMA'] = df['close'].rolling(window=14).mean()
df.fillna(0, inplace=True)

features = ['returns', 'RSI', 'MACD', 'MACD_Signal', 'OBV', 'ATR', 'SMA', 'close']
scaler = StandardScaler()
data_scaled = scaler.fit_transform(df[features])

train_data, test_data = train_test_split(data_scaled, test_size=0.1, shuffle=False)
train_data, val_data = train_test_split(train_data, test_size=1/9, shuffle=False)

look_back = 8
train_generator = TimeseriesGenerator(train_data, train_data[:, 0], length=look_back, batch_size=20)
val_generator = TimeseriesGenerator(val_data, val_data[:, 0], length=look_back, batch_size=1)
test_generator = TimeseriesGenerator(test_data, test_data[:, 0], length=look_back, batch_size=1)

model = Sequential()
model.add(LSTM(200, activation='tanh', input_shape=(look_back, len(features)), return_sequences=True))
model.add(LSTM(200, activation='tanh'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

early_stopping = EarlyStopping(monitor='val_loss', patience=300, verbose=1, mode='min', restore_best_weights=True)
model_checkpoint = ModelCheckpoint(
    filepath='/content/drive/MyDrive/model_check/unit200/model_epoch_{epoch:02d}_val_loss_{val_loss:.4f}.h5',
    monitor='val_loss',
    mode='min',
    save_best_only=False,
    verbose=1)

history = model.fit(train_generator, epochs=300, validation_data=val_generator, callbacks=[early_stopping, model_checkpoint])

actual = df['returns'][len(train_data) + len(val_data) + look_back:].values[:len(test_generator)]
prediction = model.predict(test_generator)
prediction = prediction.flatten()

mae = mean_absolute_error(actual, prediction)
mse = mean_squared_error(actual, prediction)
rmse = np.sqrt(mse)

print(f'Mean Absolute Error (MAE): {mae}')
print(f'Mean Squared Error (MSE): {mse}')
print(f'Root Mean Squared Error (RMSE): {rmse}')

# 변동률 학습
import pandas as pd
from keras.models import Sequential
from keras.layers import LSTM, Dense
import numpy as np
from keras.preprocessing.sequence import TimeseriesGenerator
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from keras.callbacks import ModelCheckpoint
from keras.layers import Dropout
from sklearn.preprocessing import StandardScaler

df = pd.read_csv(r"/content/drive/MyDrive/btc_usdt_future_1h_2021.csv")

df['returns'] = df['close'].pct_change()

df = df.dropna()
df_returns = df[['returns']].values

scaler = StandardScaler()
data_scaled = scaler.fit_transform(df_returns)

train_data, test_data = train_test_split(data_scaled, test_size=0.1, shuffle=False)
train_data, val_data = train_test_split(train_data, test_size=1/9, shuffle=False)
look_back = 8
train_generator = TimeseriesGenerator(train_data, train_data[:, 0], length=look_back, batch_size=20)
val_generator = TimeseriesGenerator(val_data, val_data[:, 0], length=look_back, batch_size=1)
test_generator = TimeseriesGenerator(test_data, test_data[:, 0], length=look_back, batch_size=1)

model = Sequential()
model.add(LSTM(50, activation='tanh', input_shape=(look_back, 1), return_sequences=True))
model.add(LSTM(50, activation='tanh'))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')

from keras.callbacks import EarlyStopping, ModelCheckpoint

early_stopping = EarlyStopping(monitor='val_loss', patience=1000, verbose=1, mode='min', restore_best_weights=True)

model_checkpoint = ModelCheckpoint(
    filepath='model_epoch_{epoch:02d}_val_loss_{val_loss:.4f}.tf',
    monitor='val_loss',
    mode='min',
    save_best_only=False,
    verbose=1,
)

history = model.fit(
    train_generator,
    epochs=200,
    verbose=1,
    validation_data=val_generator,
    callbacks=[early_stopping, model_checkpoint]
)

actual = df_returns[len(train_data) + len(val_data) + look_back:][:len(test_generator)]
prediction = model.predict(test_generator)
prediction = scaler.inverse_transform(prediction)

mae = mean_absolute_error(actual, prediction)
mse = mean_squared_error(actual, prediction)
rmse = np.sqrt(mse)
print(f'Mean Absolute Error (MAE): {mae}')
print(f'Mean Squared Error (MSE): {mse}')
print(f'Root Mean Squared Error (RMSE): {rmse}')

#6k 분류학습
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing.sequence import TimeseriesGenerator
from sklearn.metrics import accuracy_score
import joblib

def calculate_rsi(data, window=14):
    delta = data.diff()
    gain = (delta > 0) * delta
    loss = (delta < 0) * -delta
    avg_gain = gain.rolling(window=window, min_periods=1).mean()
    avg_loss = loss.rolling(window=window, min_periods=1).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

def calculate_macd(data, short_window=12, long_window=26, signal_window=9):
    short_ema = data.ewm(span=short_window, adjust=False).mean()
    long_ema = data.ewm(span=long_window, adjust=False).mean()
    macd = short_ema - long_ema
    signal_line = macd.ewm(span=signal_window, adjust=False).mean()
    return macd, signal_line

def calculate_obv(data, volume):
    obv = np.zeros_like(data)
    obv[0] = volume.iloc[0]
    for i in range(1, len(data)):
        if data.iloc[i] > data.iloc[i-1]:
            obv[i] = obv[i-1] + volume.iloc[i]
        elif data.iloc[i] < data.iloc[i-1]:
            obv[i] = obv[i-1] - volume.iloc[i]
        else:
            obv[i] = obv[i-1]
    return pd.Series(obv, index=data.index)


df = pd.read_csv(r"/content/drive/MyDrive/btc_usdt_future_1h_2021.csv")

df['returns'] = df['close'].pct_change()
df['target'] = np.where(df['returns'] > 0, 1, 0)
df['RSI'] = calculate_rsi(df['close'])
macd, signal_line = calculate_macd(df['close'])
df['MACD_Signal'] = signal_line
df['OBV'] = calculate_obv(df['close'], df['volume'])
df['SMA'] = df['close'].rolling(window=14).mean()

df.fillna(0, inplace=True)

features = df[['RSI','MACD_Signal', 'OBV', 'SMA', 'close', 'returns']].values
targets = df['target'].values

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)
scaler_filename = "/content/drive/MyDrive/model_check_ag/6k_scaler.save"
joblib.dump(scaler, scaler_filename)
print("Scaler saved at", scaler_filename)

train_features, test_features, train_targets, test_targets = train_test_split(features_scaled, targets, test_size=0.2, shuffle=False)
train_features, val_features, train_targets, val_targets = train_test_split(train_features, train_targets, test_size=0.25, shuffle=False)

look_back = 24
train_generator = TimeseriesGenerator(train_features, train_targets, length=look_back, batch_size=20)
val_generator = TimeseriesGenerator(val_features, val_targets, length=look_back, batch_size=1)
test_generator = TimeseriesGenerator(test_features, test_targets, length=look_back, batch_size=1)
model = Sequential()
model.add(LSTM(150, activation='tanh', input_shape=(look_back, 6), return_sequences=True))
model.add(LSTM(150, activation='tanh'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=2000, verbose=1, mode='min', restore_best_weights=True)
model_checkpoint = ModelCheckpoint(
    filepath="/content/drive/MyDrive/model_check_ag/segment_6k/unit150_back24/model_epoch_{epoch:02d}.h5",
    monitor='val_loss',
    mode='min',
    save_best_only=False,
    verbose=1)

history = model.fit(train_generator, epochs=100, validation_data=val_generator, callbacks=[early_stopping, model_checkpoint])

predictions = model.predict(test_generator).flatten()
predictions = (predictions > 0.5).astype(int)
accuracy = accuracy_score(test_targets[look_back:], predictions)

print(f'Accuracy: {accuracy}')

#2k 분류학습
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing.sequence import TimeseriesGenerator
from sklearn.metrics import accuracy_score
import joblib

df = pd.read_csv(r"/content/drive/MyDrive/btc_usdt_future_1h_2021.csv")


df['returns'] = df['close'].pct_change()
df['target'] = np.where(df['returns'] > 0, 1, 0)

df.fillna(0, inplace=True)

features = df[['close', 'returns']].values
targets = df['target'].values

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)
scaler_filename = "/content/drive/MyDrive/model_check_ag/2k_scaler.save"
joblib.dump(scaler, scaler_filename)
print("Scaler saved at", scaler_filename)

train_features, test_features, train_targets, test_targets = train_test_split(features_scaled, targets, test_size=0.2, shuffle=False)
train_features, val_features, train_targets, val_targets = train_test_split(train_features, train_targets, test_size=0.25, shuffle=False)


look_back = 24
train_generator = TimeseriesGenerator(train_features, train_targets, length=look_back, batch_size=20)
val_generator = TimeseriesGenerator(val_features, val_targets, length=look_back, batch_size=1)
test_generator = TimeseriesGenerator(test_features, test_targets, length=look_back, batch_size=1)

model = Sequential()
model.add(LSTM(150, activation='tanh', input_shape=(look_back, 2), return_sequences=True))
model.add(LSTM(150, activation='tanh'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=2000, verbose=1, mode='min', restore_best_weights=True)
model_checkpoint = ModelCheckpoint(
    filepath="/content/drive/MyDrive/model_check_ag/segment_2k/unit150_back24/model_epoch_{epoch:02d}.h5",
    monitor='val_loss',
    mode='min',
    save_best_only=False,
    verbose=1)

history = model.fit(train_generator, epochs=100, validation_data=val_generator, callbacks=[early_stopping, model_checkpoint])

predictions = model.predict(test_generator).flatten()
predictions = (predictions > 0.5).astype(int)
accuracy = accuracy_score(test_targets[look_back:], predictions)

print(f'Accuracy: {accuracy}')

# 4k 분류학습
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing.sequence import TimeseriesGenerator
from sklearn.metrics import accuracy_score
import joblib
df = pd.read_csv(r"/content/drive/MyDrive/btc_usdt_future_1h_2021.csv")

df['returns'] = df['close'].pct_change()
df['target'] = np.where(df['returns'] > 0, 1, 0)

df.fillna(0, inplace=True)

features = df[['close', 'returns','high','low']].values
targets = df['target'].values

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)
scaler_filename = "/content/drive/MyDrive/model_check_ag/4k_scaler.save"
joblib.dump(scaler, scaler_filename)
print("Scaler saved at", scaler_filename)
train_features, test_features, train_targets, test_targets = train_test_split(features_scaled, targets, test_size=0.2, shuffle=False)
train_features, val_features, train_targets, val_targets = train_test_split(train_features, train_targets, test_size=0.25, shuffle=False)


look_back = 48
train_generator = TimeseriesGenerator(train_features, train_targets, length=look_back, batch_size=20)
val_generator = TimeseriesGenerator(val_features, val_targets, length=look_back, batch_size=1)
test_generator = TimeseriesGenerator(test_features, test_targets, length=look_back, batch_size=1)


model = Sequential()
model.add(LSTM(150, activation='tanh', input_shape=(look_back, 4), return_sequences=True))
model.add(LSTM(150, activation='tanh'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=2000, verbose=1, mode='min', restore_best_weights=True)
model_checkpoint = ModelCheckpoint(
    filepath="/content/drive/MyDrive/model_check_ag/segment_4k/unit150_back48/model_epoch_{epoch:02d}.h5",
    monitor='val_loss',
    mode='min',
    save_best_only=False,
    verbose=1)

history = model.fit(train_generator, epochs=100, validation_data=val_generator, callbacks=[early_stopping, model_checkpoint])


predictions = model.predict(test_generator).flatten()
predictions = (predictions > 0.5).astype(int)
accuracy = accuracy_score(test_targets[look_back:], predictions)

print(f'Accuracy: {accuracy}')

# ak_no_test 분류학습
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing.sequence import TimeseriesGenerator
from sklearn.metrics import accuracy_score
import joblib
df = pd.read_csv(r"/content/drive/MyDrive/btc_usdt_future_1h_2021.csv")


df['returns'] = df['close'].pct_change()
df['target'] = np.where(df['returns'] > 0, 1, 0)

df.fillna(0, inplace=True)

features = df[['close', 'returns','high','low']].values
targets = df['target'].values

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)
scaler_filename = "/content/drive/MyDrive/1213model_ag_no_test/4k_scaler.save"
joblib.dump(scaler, scaler_filename)
print("Scaler saved at", scaler_filename)

train_features, val_features, train_targets, val_targets = train_test_split(features_scaled, targets, test_size=0.2, shuffle=False)


look_back = 24
train_generator = TimeseriesGenerator(train_features, train_targets, length=look_back, batch_size=20)
val_generator = TimeseriesGenerator(val_features, val_targets, length=look_back, batch_size=1)


model = Sequential()
model.add(LSTM(100, activation='tanh', input_shape=(look_back, 4), return_sequences=True))
model.add(LSTM(100, activation='tanh'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=2000, verbose=1, mode='min', restore_best_weights=True)
model_checkpoint = ModelCheckpoint(
    filepath="/content/drive/MyDrive/model_ag_no_test/1213segment_4k/unit100_back24/model_epoch_{epoch:02d}.h5",
    monitor='val_loss',
    mode='min',
    save_best_only=False,
    verbose=1)
history = model.fit(train_generator, epochs=100, validation_data=val_generator, callbacks=[early_stopping, model_checkpoint])

#6k 정확도 계산

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing.sequence import TimeseriesGenerator
import joblib

def calculate_rsi(data, window=14):
    delta = data.diff()
    gain = delta.where(delta > 0, 0)
    loss = (-delta).where(delta < 0, 0)

    avg_gain = gain.rolling(window=window, min_periods=1).mean()
    avg_loss = loss.rolling(window=window, min_periods=1).mean()

    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

def calculate_macd(data, short_window=12, long_window=26, signal_window=9):
    short_ema = data.ewm(span=short_window, adjust=False).mean()
    long_ema = data.ewm(span=long_window, adjust=False).mean()
    macd = short_ema - long_ema
    signal_line = macd.ewm(span=signal_window, adjust=False).mean()
    return macd, signal_line

def calculate_obv(data, volume):
    obv = np.where(data > data.shift(1), volume, -volume).cumsum()
    return obv

def calculate_atr(high, low, close, window=14):
    high_low = high - low
    high_close = np.abs(high - close.shift())
    low_close = np.abs(low - close.shift())
    ranges = pd.concat([high_low, high_close, low_close], axis=1)
    true_range = np.max(ranges, axis=1)
    atr = true_range.rolling(window=window).mean()
    return atr

df = pd.read_csv(r"/content/drive/MyDrive/btc_usdt_future_1h_2021.csv")
df['returns'] = df['close'].pct_change()
df['RSI'] = calculate_rsi(df['close'])
macd, signal_line = calculate_macd(df['close'])
df['MACD'] = macd
df['MACD_Signal'] = signal_line
df['OBV'] = calculate_obv(df['close'], df['volume'])
df['ATR'] = calculate_atr(df['high'], df['low'], df['close'])
df['SMA'] = df['close'].rolling(window=14).mean()
df.fillna(0, inplace=True)


features = df[['close', 'returns','RSI','MACD_Signal','OBV','SMA']].values
scaler = joblib.load("/content/drive/MyDrive/model_check_ag/6k_scaler.save")
data_scaled = scaler.transform(features)

df['target'] = np.where(df['returns'] > 0, 1, 0)
targets = df['target'].values

train_features, test_features, train_targets, test_targets = train_test_split(features, targets, test_size=0.2, shuffle=False)
train_features, val_features, train_targets, val_targets = train_test_split(train_features, train_targets, test_size=0.25, shuffle=False)

look_back = 24
train_generator = TimeseriesGenerator(train_features, train_targets, length=look_back, batch_size=20)
val_generator = TimeseriesGenerator(val_features, val_targets, length=look_back, batch_size=1)
test_generator = TimeseriesGenerator(test_features, test_targets, length=look_back, batch_size=1)


import os
from keras.models import load_model
from sklearn.metrics import accuracy_score

model_folder_path = "/content/drive/MyDrive/model_check_ag/segment_6k/unit150_back24"

model_files = [os.path.join(model_folder_path, f) for f in os.listdir(model_folder_path) if f.endswith('.h5')]


def get_direction(array):
    return np.array([1 if array[i] > array[i - 1] else 0 for i in range(1, len(array))])


for model_path in model_files:
    model = load_model(model_path)
    prediction = model.predict(test_generator).flatten()
    actual_returns = test_features[look_back:, 1]
    actual_direction = get_direction(actual_returns)
    prediction_direction = get_direction(prediction)

    direction_accuracy = accuracy_score(actual_direction, prediction_direction)

    print(f'Model: {os.path.basename(model_path)} - Direction Accuracy: {direction_accuracy*100:.2f}%')

#4k 정확도 계산

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing.sequence import TimeseriesGenerator
import joblib

df = pd.read_csv(r"/content/drive/MyDrive/btc_usdt_future_1h_2021.csv")
df['returns'] = df['close'].pct_change()
df.fillna(0, inplace=True)

features = df[['close', 'returns','high','low']].values
scaler = joblib.load("/content/drive/MyDrive/model_check_ag/4k_scaler.save")
data_scaled = scaler.transform(features)

df['target'] = np.where(df['returns'] > 0, 1, 0)
targets = df['target'].values

train_features, test_features, train_targets, test_targets = train_test_split(features, targets, test_size=0.2, shuffle=False)
train_features, val_features, train_targets, val_targets = train_test_split(train_features, train_targets, test_size=0.25, shuffle=False)

look_back = 24
train_generator = TimeseriesGenerator(train_features, train_targets, length=look_back, batch_size=20)
val_generator = TimeseriesGenerator(val_features, val_targets, length=look_back, batch_size=1)
test_generator = TimeseriesGenerator(test_features, test_targets, length=look_back, batch_size=1)


import os
from keras.models import load_model
from sklearn.metrics import accuracy_score

model_folder_path = "/content/drive/MyDrive/model_check_ag/segment_4k/unit150_back48"
print(model_folder_path)
model_files = [os.path.join(model_folder_path, f) for f in os.listdir(model_folder_path) if f.endswith('.h5')]

def get_direction(array):
    return np.array([1 if array[i] > array[i - 1] else 0 for i in range(1, len(array))])

for model_path in model_files:
    model = load_model(model_path)
    prediction = model.predict(test_generator).flatten()

    actual_returns = test_features[look_back:, 1]
    actual_direction = get_direction(actual_returns)
    prediction_direction = get_direction(prediction)

    direction_accuracy = accuracy_score(actual_direction, prediction_direction)

    print(f'Model: {os.path.basename(model_path)} - Direction Accuracy: {direction_accuracy*100:.2f}%')

#2k 정확도 계산

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing.sequence import TimeseriesGenerator
import joblib


df = pd.read_csv(r"/content/drive/MyDrive/btc_usdt_future_1h_2021.csv")
df['returns'] = df['close'].pct_change()
df.fillna(0, inplace=True)

features = df[['close', 'returns']].values
scaler = joblib.load("/content/drive/MyDrive/2k_scaler.save")
data_scaled = scaler.transform(features)

df['target'] = np.where(df['returns'] > 0, 1, 0)
targets = df['target'].values

train_features, test_features, train_targets, test_targets = train_test_split(features, targets, test_size=0.2, shuffle=False)
train_features, val_features, train_targets, val_targets = train_test_split(train_features, train_targets, test_size=0.25, shuffle=False)

look_back = 8
train_generator = TimeseriesGenerator(train_features, train_targets, length=look_back, batch_size=20)
val_generator = TimeseriesGenerator(val_features, val_targets, length=look_back, batch_size=1)
test_generator = TimeseriesGenerator(test_features, test_targets, length=look_back, batch_size=1)


import os
from keras.models import load_model
from sklearn.metrics import accuracy_score

model_folder_path = "/content/drive/MyDrive/model_check_ag/segment_2k/unit50_back8"
model_files = [os.path.join(model_folder_path, f) for f in os.listdir(model_folder_path) if f.endswith('.h5')]

def get_direction(array):
    return np.array([1 if array[i] > array[i - 1] else 0 for i in range(1, len(array))])

for model_path in model_files:
    model = load_model(model_path)

    prediction = model.predict(test_generator).flatten()
    actual_returns = test_features[look_back:, 1]
    actual_direction = get_direction(actual_returns)
    prediction_direction = get_direction(prediction)
    direction_accuracy = accuracy_score(actual_direction, prediction_direction)

    print(f'Model: {os.path.basename(model_path)} - Direction Accuracy: {direction_accuracy*100:.2f}%')

#일반 지표 분류학습
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing.sequence import TimeseriesGenerator
from sklearn.metrics import accuracy_score


df = pd.read_csv(r"/content/drive/MyDrive/btc_usdt_future_1h_2021.csv")

df['returns'] = df['close'].pct_change()
df['target'] = np.where(df['returns'] > 0, 1, 0)

df.fillna(0, inplace=True)
features = df[['close', 'returns']].values
targets = df['target'].values

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

train_features, val_features, train_targets, val_targets = train_test_split(features_scaled, targets, test_size=0.3, shuffle=False)

look_back = 8
train_generator = TimeseriesGenerator(train_features, train_targets, length=look_back, batch_size=20)
val_generator = TimeseriesGenerator(val_features, val_targets, length=look_back, batch_size=1)

model = Sequential()
model.add(LSTM(50, activation='tanh', input_shape=(look_back, 2), return_sequences=True))
model.add(LSTM(50, activation='tanh'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=2000, verbose=1, mode='min', restore_best_weights=True)
model_checkpoint = ModelCheckpoint(
    filepath="/content/drive/MyDrive/model_check/segment_2k_noval/model_epoch_{epoch:02d}.h5",
    monitor='val_loss',
    mode='min',
    save_best_only=False,
    verbose=1)

history = model.fit(train_generator, epochs=100, validation_data=val_generator, callbacks=[early_stopping, model_checkpoint])

unique, counts = np.unique(train_targets, return_counts=True)
class_distribution = dict(zip(unique, counts))
print("Class distribution in test data:", class_distribution)

total_samples = sum(counts)
for cls, count in class_distribution.items():
    print(f"Class {cls}: {count} samples, {count / total_samples * 100:.2f}% of the test set")

import pandas as pd
from sklearn.preprocessing import StandardScaler
from joblib import dump

df = pd.read_csv(r"/content/drive/MyDrive/btc_usdt_future_1h_2021.csv")
df_close = df[['close']].values

scaler = StandardScaler()
data_scaled = scaler.fit_transform(df_close)

dump(scaler, '/content/drive/MyDrive/scaler_btc_usdt_future_1h_2021.joblib')

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from keras.models import load_model
import numpy as np
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import TimeseriesGenerator
import joblib

df = pd.read_csv(r"/content/drive/MyDrive/btc_usdt_future_1h_2021.csv")
df_close = df[['close']].values
scaler_filename = '/content/drive/MyDrive/scaler.joblib'
scaler = joblib.load(scaler_filename)
data_scaled = scaler.fit_transform(df_close)
train_data, test_data = train_test_split(data_scaled, test_size=0.2, shuffle=False)
look_back = 48

test_generator = TimeseriesGenerator(test_data, test_data[:, 0], length=look_back, batch_size=1)

model_file_path = '/content/drive/MyDrive/st/coin_model_unit50_hour48_st.tf'

print(f'Evaluating model: {model_file_path}')
model = load_model(model_file_path)

prediction = model.predict(test_generator)
prediction = scaler.inverse_transform(prediction)
actual = scaler.inverse_transform(test_data[look_back:])

mae = mean_absolute_error(actual, prediction)
mse = mean_squared_error(actual, prediction)
rmse = np.sqrt(mse)

print(f'MAE: {mae}, MSE: {mse}, RMSE: {rmse}\n')

import matplotlib.pyplot as plt
test_index = df.index[len(train_data) + len(val_data) + look_back:]

plt.figure(figsize=(15, 7))
plt.plot(test_index, actual, label='Actual Price', color='blue')
plt.plot(test_index, prediction, label='Predicted Price', color='red', linestyle='--')

plt.title('Bitcoin Price Prediction')
plt.xlabel('Time')
plt.ylabel('Price')

plt.legend()
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.metrics import mean_squared_error, mean_absolute_error
from keras.preprocessing.sequence import TimeseriesGenerator
import matplotlib.pyplot as plt
from keras.models import load_model

file_path = '/content/drive/MyDrive/coin/btc_future=(0302.1635).csv'
df = pd.read_csv(file_path, parse_dates=['timestamp'], index_col='timestamp')

df_close = df[['close']]

scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(df_close)

data_train = data_scaled
look_back = 10
batch_size = 1
train_generator = TimeseriesGenerator(data_train, data_train, length=look_back, batch_size=batch_size)
model = load_model('/content/drive/MyDrive/coin/best_model.h5')


last_batch = data_train[-look_back:]
current_batch = last_batch.reshape((1, look_back, 1))

predictions = []

for i in range(16):
    current_pred = model.predict(current_batch)[0]
    predictions.append(current_pred)
    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)

predictions = scaler.inverse_transform(predictions)

last_timestamp = df.index[-1]
prediction_timestamps = [last_timestamp + pd.Timedelta(hours=x+1+9) for x in range(16)]

plt.figure(figsize=(10, 6))
plt.plot(prediction_timestamps, predictions, label='Predicted Price')
plt.title('Bitcoin Price Prediction for the Next 16 Hours')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.show()

last_date = df.index[-1]

prediction_dates = [last_date + pd.Timedelta(hours=x+9) for x in range(1, 17)]
predictions_df = pd.DataFrame(data=predictions, index=prediction_dates, columns=['Predicted Price'])
predictions_df.to_csv('pred.csv')


print("CSV file with predictions saved: predictions.csv")
